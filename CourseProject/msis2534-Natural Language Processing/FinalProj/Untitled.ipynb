{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f808706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
      "     |████████████████████████████████| 4.0 MB 24.8 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (4.62.3)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
      "     |████████████████████████████████| 880 kB 64.1 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (2.26.0)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.4.1-py3-none-any.whl (9.9 kB)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (3.7.0)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp36-cp36m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "     |████████████████████████████████| 6.6 MB 56.0 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (5.4.1)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "     |████████████████████████████████| 67 kB 8.3 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers) (2.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895254 sha256=43b63edc28a16633420946ce895391ed7c64b957b7d12c151203d7e9d4e640b1\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/4c/64/31/e9900a234b23fb3e9dc565d6114a9d6ff84a72dbdd356502b4\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: filelock, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.4.1 huggingface-hub-0.4.0 sacremoses-0.0.53 tokenizers-0.12.1 transformers-4.18.0\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.2.2-py3-none-any.whl (346 kB)\n",
      "     |████████████████████████████████| 346 kB 23.5 MB/s            \n",
      "\u001b[?25hCollecting fsspec[http]>=2021.05.0\n",
      "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
      "     |████████████████████████████████| 133 kB 71.9 MB/s            \n",
      "\u001b[?25hCollecting responses<0.19\n",
      "  Downloading responses-0.17.0-py2.py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets) (0.8)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets) (2.26.0)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets) (1.1.5)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets) (1.19.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets) (4.62.3)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
      "     |████████████████████████████████| 211 kB 69.8 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets) (3.7.0)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: dill<0.3.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (5.4.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (2.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (3.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from responses<0.19->datasets) (1.15.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from aiohttp->datasets) (20.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from aiohttp->datasets) (4.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: idna-ssl>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from aiohttp->datasets) (1.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from importlib-metadata->datasets) (3.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pandas->datasets) (2021.1)\n",
      "Installing collected packages: fsspec, xxhash, responses, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2021.4.0\n",
      "    Uninstalling fsspec-2021.4.0:\n",
      "      Successfully uninstalled fsspec-2021.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "s3fs 2021.4.0 requires fsspec==2021.04.0, but you have fsspec 2022.1.0 which is incompatible.\u001b[0m\n",
      "Successfully installed datasets-2.2.2 fsspec-2022.1.0 responses-0.17.0 xxhash-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d640c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74536f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c303ee91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aebb876",
   "metadata": {},
   "source": [
    "## Load data set & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29afecc2",
   "metadata": {},
   "source": [
    "+ Won't remove stop word here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c617cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation, remove marks, remove urls, lower case,remove numbers, lowercase\n",
    "#remove ascii chatachters\n",
    "def removePuncStr(s):\n",
    "    for c in string.punctuation + 'Ã'+'±'+'ã'+'¼'+'â'+'»'+'§':\n",
    "        s = s.replace(c, ' ').strip()\n",
    "    return s\n",
    "\n",
    "def removePunc(array):\n",
    "    return [removePuncStr(str_) for str_ in array]\n",
    "\n",
    "#remove numbers\n",
    "def removeNumbersStr(s):\n",
    "    for d in range(10):\n",
    "        s = s.replace(str(d), ' ')\n",
    "    return s\n",
    "\n",
    "def removeNumbers(array):\n",
    "    return [removeNumbersStr(str_) for str_ in array]\n",
    "\n",
    "def lemmatize(text_array):\n",
    "    WNlemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_text = []\n",
    "    for h in text_array:\n",
    "        words = word_tokenize(h)\n",
    "        h2 = ''\n",
    "        for w in words:\n",
    "            h2 = h2 + ' ' + WNlemmatizer.lemmatize(w)\n",
    "        lemmatized_text.append(h2)\n",
    "    return lemmatized_text\n",
    "\n",
    "def removeSpacesStr(s):\n",
    "    spaces = ['\\t', '\\r', '\\n']\n",
    "    for space in spaces:\n",
    "        s = s.replace(space, ' ')\n",
    "        \n",
    "def removeSpaces(array):\n",
    "    return [removeNumbersStr(sent) for sent in array]\n",
    "\n",
    "def Lowercase(array):\n",
    "    return [str(sent).lower() for sent in array]\n",
    "\n",
    "def removeUrl(text_array):\n",
    "    pattern = r\"https://.+\\S*\"\n",
    "    removed = []\n",
    "    for s in text_array:\n",
    "        s = re.sub(pattern, ' ', s)\n",
    "        removed.append(s)\n",
    "    return removed\n",
    "def removeAscii(text_array):\n",
    "    return [re.sub(r'[^\\x00-\\x7f]', '', s) for s in text_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95d7f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text_array):\n",
    "    text_array = removeUrl(text_array)\n",
    "    text_array = removePunc(text_array)\n",
    "    text_array = removeNumbers(text_array)\n",
    "    text_array = removeSpaces(text_array)\n",
    "    text_array = Lowercase(text_array)\n",
    "    text_array = lemmatize(text_array)\n",
    "    text_array = removeAscii(text_array)\n",
    "    return text_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9fed8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapper = {\n",
    "    'Neutral' : 0,\n",
    "    'Positive' : 1,\n",
    "    'Extremely Positive' : 2,\n",
    "    'Negative' : 3,\n",
    "    'Extremely Negative' : 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6f73ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for initializing a dataset for Bert\n",
    "def makeDataset(dataset):\n",
    "    dataset = dataset[['OriginalTweet', 'Sentiment']]\n",
    "    text_array = dataset['OriginalTweet']\n",
    "    text_array = clean(text_array)\n",
    "    dataset['Text'] = text_array\n",
    "    dataset = dataset.drop(columns=['OriginalTweet'])\n",
    "    dataset = dataset.rename(columns = {'Sentiment': 'Labels'})\n",
    "    dataset['Labels'] = dataset['Labels'].map(label_mapper)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30560b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "train_set = pd.read_csv('Corona_NLP_train.csv', encoding='ISO-8859-1')\n",
    "test_set = pd.read_csv('Corona_NLP_test.csv', encoding='ISO-8859-1')\n",
    "\n",
    "train_set = makeDataset(train_set)\n",
    "test_set = makeDataset(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f40fcfd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Labels</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>menyrbie phil gahan chrisitv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>advice talk to your neighbour family to excha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>coronavirus australia woolworth to give elder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>my food stock is not the only one which is em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>me ready to go at supermarket during the covi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Labels                                               Text\n",
       "0       0                       menyrbie phil gahan chrisitv\n",
       "1       1   advice talk to your neighbour family to excha...\n",
       "2       1   coronavirus australia woolworth to give elder...\n",
       "3       1   my food stock is not the only one which is em...\n",
       "4       4   me ready to go at supermarket during the covi..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90d8fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_data = Dataset.from_pandas(train_set)\n",
    "test_data = Dataset.from_pandas(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38bf01c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_id = {\n",
    "    'Neutral' : 0,\n",
    "    'Positive' : 1,\n",
    "    'Extremely Positive' : 2,\n",
    "    'Negative' : 3,\n",
    "    'Extremely Negative' : 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff6d0966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Neutral',\n",
       " 1: 'Positive',\n",
       " 2: 'Extremely Positive',\n",
       " 3: 'Negative',\n",
       " 4: 'Extremely Negative'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {id_:name for name, id_ in label_id.items()}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ffcac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splits train and validation set\n",
    "#use train_test_split in Datasets module\n",
    "\n",
    "splitted = train_data.train_test_split(test_size=0.2, shuffle=True, seed=222)\n",
    "train_data, val_data = splitted['train'], splitted['test']\n",
    "\n",
    "from datasets import DatasetDict\n",
    "datasets = DatasetDict({'train' : train_data, 'val' : val_data, 'test' : test_data})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ce2d5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Labels', 'Text'],\n",
       "        num_rows: 32925\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['Labels', 'Text'],\n",
       "        num_rows: 8232\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Labels', 'Text'],\n",
       "        num_rows: 3798\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a4437b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a964d6dca9461fb83073e6cf0e6b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf524e2e06a4b15b1117823c7495d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c90def7073a04e578f403005919d757e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a359b70072433c99f38729fcf0690d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Load Bert Model and Tokenizer\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "bertModel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_id), id2label=id2label, label2id=label_id)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43d12024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      " i want to go shopping i want to walk w o the thought i might get the covid virus i want a real hug i want to go eat at a dine in restaurant ohhh to be free w o the thought of that darn virus i want to go shopping for shoe i don t need just want to go shopping no more online\n"
     ]
    }
   ],
   "source": [
    "## find max sequence length\n",
    "max_length = 0\n",
    "max_text = ''\n",
    "\n",
    "for text in train_set['Text']:\n",
    "    if(len(text.split(' ')) > max_length):\n",
    "        max_length = len(text.split(' '))\n",
    "        max_text = text\n",
    "print(max_length)\n",
    "print(max_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19c1d1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de21ed81d10a4ad8ac75b440e42a6ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenization:   0%|          | 0/33 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd3c19b2e274297ade3ccee935e9092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenization:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d275b7f92d0f4b89be8904e931124729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenization:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 5s, sys: 1.05 s, total: 1min 6s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "max_sequence_length = 128\n",
    "\n",
    "def tokenization(dataset):\n",
    "    \n",
    "    args = ((dataset['Text'], ))\n",
    "    result = tokenizer(*args, padding='max_length', max_length=max_sequence_length, truncation=True)\n",
    "    result['label'] = [label for label in dataset['Labels']]\n",
    "    \n",
    "    return result\n",
    "\n",
    "datasets = datasets.map(tokenization, batched=True, desc='Tokenization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a783aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Labels', 'Text', 'input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
       "    num_rows: 32925\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51dbec41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys:  dict_keys(['Labels', 'Text', 'input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
      "text:   overheard at the grocery store i like frozen food better than fresh anyways  boyyyy do i have an ex for you girl coronavirus\n",
      "128\n",
      "tokenized sequence : [CLS] overheard at the grocery store i like frozen food better than fresh anyways boyyyy do i have an ex for you girl coronavirus [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "end-----------------------------------------------------------------------\n",
      "keys:  dict_keys(['Labels', 'Text', 'input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
      "text:   an ongoing oil price war and the weakened economy due to covid are driving price down\n",
      "128\n",
      "tokenized sequence : [CLS] an ongoing oil price war and the weakened economy due to covid are driving price down [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "end-----------------------------------------------------------------------\n",
      "keys:  dict_keys(['Labels', 'Text', 'input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
      "text:   i notice some medium amp ppl talking a if all supermarket shelf are near empty soon after opening this is misleading some shelf are empty many others are well stocked amp if you know how to cook anything other than pasta with pre made sauce you should be fine\n",
      "128\n",
      "tokenized sequence : [CLS] i notice some medium amp ppl talking a if all supermarket shelf are near empty soon after opening this is misleading some shelf are empty many others are well stocked amp if you know how to cook anything other than pasta with pre made sauce you should be fine [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "end-----------------------------------------------------------------------\n",
      "keys:  dict_keys(['Labels', 'Text', 'input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
      "text:   no need to worry about going to the supermarket and scramble for the last laundry product we deliver all over the u k with free delivery for all order of  or over visit our website and order today selfisolation covid\n",
      "128\n",
      "tokenized sequence : [CLS] no need to worry about going to the supermarket and scramble for the last laundry product we deliver all over the u k with free delivery for all order of or over visit our website and order today selfisolation covid [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "end-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(datasets['train'])):\n",
    "    if(i<=3):\n",
    "        print('keys: ', datasets['train'][i].keys())\n",
    "        print('text: ', datasets['train'][i]['Text'])\n",
    "        print(len(datasets['train'][i]['input_ids']))\n",
    "        print('tokenized sequence :', tokenizer.decode(datasets['train'][i]['input_ids']))\n",
    "        print('end-----------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d00adf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a7800c97d84bb7b3dace45da9ba577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Training\n",
    "\n",
    "from datasets import load_metric\n",
    "\n",
    "metric_name = 'accuracy'\n",
    "metric = load_metric(metric_name)\n",
    "\n",
    "def compute_metrics(returned):\n",
    "    y_pred = returned.predictions[0] if isinstance(returned.predictions, tuple) else returned.predictions\n",
    "    y_pred = np.argmax(y_preds, axis=1)\n",
    "    \n",
    "    return {\"accuracy\" : (y_pred == returned.label_ids).astype(np.float32).mean().item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea409b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set Training Arguments\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "'Corona_NLP_Bert',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate= 2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "798c02f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize Trainer\n",
    "debug = False\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = bertModel,\n",
    "    args = training_args,\n",
    "    train_dataset=datasets['train'].select(range(10)) if debug else datasets['train'],\n",
    "    eval_dataset = datasets['val'].select(range(10)) if debug else datasets['val'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a44c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text, Labels. If Text, Labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 32925\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4116\n"
     ]
    }
   ],
   "source": [
    "## Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb24a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940f6882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set = datasets['test']\n",
    "# test_set = test_set.remove_columns(\"Labels\")\n",
    "# y_pred = trainer.predict(test_set, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
